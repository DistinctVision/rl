model:
  memory_size: 30
  
  rnn:
    action_dim: 32
    inner_dim: 1024
    hidden_dim: 1024
    n_lstm_layers: 2

    state_predictor_path: '../best_state_predictor.kpt'
    state_predictor_optimizer_path: '../rl_output/state_predictor_10_27_2023__00_58_03/weights/opt_state_predictor_230.kpt'

  layers: [256, 128]
  dropout: 0.0
  reward_decay: 0.99

  _critic_model_path: '../rl_output/dqn_10_22_2023__20_13_43/weights/best_daft_quick_nick.kpt'
  _critic_optimizer_path: '../rl_output/dqn_10_22_2023__20_13_43/weights/opt_daft_quick_nick_500.kpt'

game:
  fps: 10

replay_buffer:
  max_buffer_size: 500000
  min_buffer_size: 1000
  _data_dir: ../gym_output/dqn_10_19_2023__15_29_00/weights

training:
  train_freq: 8
  is_double:  True
  lr: 1e-4
  
  batch_size: 256
  n_grad_accum_steps: 1
  grad_norm: 1.0
  fp16: False

  n_local_steps: 1000
  model_update:
    n_steps: 5
    type: soft
    rate: 1e-1

  output_folder: ../rl_output
  
  save:
    model_name: daft_quick_nick
    save_every_n_step: 10
    n_last_steps: 100
    target_metric: reward
    target_op: '>'
    
  eps_greedy:
    eps_from: 0.95
    eps_to: 0.05
    n_epochs_of_decays: 1000000

state_predictor_training:
  train_freq: 16000
  lr: 3e-4
  eps_greedy: 0.01
  
  batch_size: 256
  train_size:
    min: 256000
    max: 1024000
    for_epoch: 256000
  val_size:
    min: 25600
    max: 102400
    for_epoch: 256000
  
  n_grad_accum_steps: 1
  grad_norm: 1.0
  fp16: False

  n_local_steps: 1000

  output_folder: ../rl_output
  
  save:
    model_name: state_predictor
    save_every_n_step: 10
    n_last_steps: 100
    target_metric: val.num_loss
    target_op: '<'
